{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airlines Data Cleaning and Ingestion\n",
    "\n",
    "The purpose of this notebook is show some of my findings regarding the data, develop data cleaning strategies, and get to look at some of the analytical questions at the bottom.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Exploration\n",
    "\n",
    "Let's do some basic exploration.\n",
    "\n",
    "### Data Loading in production\n",
    "\n",
    "In the below I'm using Pandas' `read_excel()` for loading directly from Excel sheets. \n",
    "\n",
    "For a production-grade setup there should be up-front validation and additional checks to ensure everything has been loaded properly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Airline ID</th>\n",
       "      <th>Airline Name</th>\n",
       "      <th>Airline Alias</th>\n",
       "      <th>IATA Code</th>\n",
       "      <th>ICAO Code</th>\n",
       "      <th>Airline Callsign</th>\n",
       "      <th>Airline Country</th>\n",
       "      <th>Airline Operational?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>\\N</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Private flight</td>\n",
       "      <td>\\N</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>135 Airways</td>\n",
       "      <td>\\N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GNL</td>\n",
       "      <td>GENERAL</td>\n",
       "      <td>United States</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1Time Airline</td>\n",
       "      <td>\\N</td>\n",
       "      <td>1T</td>\n",
       "      <td>RNX</td>\n",
       "      <td>NEXTIME</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2 Sqn No 1 Elementary Flying Training School</td>\n",
       "      <td>\\N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WYT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UK</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Airline ID                                  Airline Name Airline Alias  \\\n",
       "0          -1                                       Unknown            \\N   \n",
       "1           1                                Private flight            \\N   \n",
       "2           2                                   135 Airways            \\N   \n",
       "3           3                                 1Time Airline            \\N   \n",
       "4           4  2 Sqn No 1 Elementary Flying Training School            \\N   \n",
       "\n",
       "  IATA Code ICAO Code Airline Callsign Airline Country Airline Operational?  \n",
       "0         -       NaN               \\N              \\N                    Y  \n",
       "1         -       NaN              NaN             NaN                    Y  \n",
       "2       NaN       GNL          GENERAL   United States                    N  \n",
       "3        1T       RNX          NEXTIME    South Africa                    Y  \n",
       "4       NaN       WYT              NaN              UK                    N  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "PROJ_PATH = \"/data/\"\n",
    "\n",
    "DATA_IN_PREFIX = \"orig/\"\n",
    "DATA_OUT_PREFIX = \"processed/\"\n",
    "\n",
    "# Load data from Excel sheets.\n",
    "airlines_raw = pd.read_excel(PROJ_PATH + DATA_IN_PREFIX + \"20180102 Airlines.xlsx\")\n",
    "airports_raw = pd.read_excel(PROJ_PATH + DATA_IN_PREFIX + \"20180102 Airports.xlsx\")\n",
    "routes_raw = pd.read_excel(PROJ_PATH + DATA_IN_PREFIX + \"20180102 Routes.xlsx\")\n",
    "airlines_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of data issues\n",
    "\n",
    "We need to distinguish two classes of data problems: Missing and invalid data.\n",
    "\n",
    "* Looks like the first row of the airlines data is invalid.\n",
    "* There are missing entries, denoted as **NaN**.\n",
    "* Other entries have the wrong format, e.g. \"\\N\", or IATA code not according to standard.\n",
    "\n",
    "Next, let's look at the datatypes and percentages of missing data in all three sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types:\n",
      "\n",
      "Airline ID               int64\n",
      "Airline Name            object\n",
      "Airline Alias           object\n",
      "IATA Code               object\n",
      "ICAO Code               object\n",
      "Airline Callsign        object\n",
      "Airline Country         object\n",
      "Airline Operational?    object\n",
      "dtype: object\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Airlines data\n",
      "\n",
      "Airline ID               0.0\n",
      "Airline Name             0.0\n",
      "Airline Alias            8.0\n",
      "IATA Code               75.0\n",
      "ICAO Code                1.0\n",
      "Airline Callsign        13.0\n",
      "Airline Country          0.0\n",
      "Airline Operational?     0.0\n",
      "dtype: float64\n",
      "\n",
      "Airports data\n",
      "\n",
      "\n",
      "Airport ID           0.0\n",
      "Airport Name         0.0\n",
      "Airport City         1.0\n",
      "Airport Country      0.0\n",
      "IATA Code            0.0\n",
      "ICAO Code            0.0\n",
      "Latitude             0.0\n",
      "Longitude            0.0\n",
      "Altitude             0.0\n",
      "Timezone Offset      4.0\n",
      "DST                  0.0\n",
      "Timezone             0.0\n",
      "Unnamed: 12        100.0\n",
      "Unnamed: 13        100.0\n",
      "dtype: float64\n",
      "\n",
      "Routes data\n",
      "\n",
      "\n",
      "Airline                    0.0\n",
      "Airline ID                 0.0\n",
      "Source Airport             0.0\n",
      "Source Airport ID          0.0\n",
      "Destination Airport        0.0\n",
      "Destination Airport ID     0.0\n",
      "Codeshare?                78.0\n",
      "No Stops                   0.0\n",
      "Equipment                  0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "## Look at airline types.\n",
    "print(\"Data types:\\n\")\n",
    "print(airlines_raw.dtypes)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# What percentages of actual missing data do we have?\n",
    "# Note: Missing means NaN; there are other types of flaws in the data which we'll get to later.\n",
    "def print_perc_missing(df):\n",
    "    \"\"\"\n",
    "    Prints what percentage of data is missing for each column.\n",
    "    \n",
    "    CAVEAT: Only checks for nan, not other data defects, e.g. \"\\\\N\" entries etc.\n",
    "    \"\"\"\n",
    "    print(round(df.isnull().sum() / df.shape[0] * 100))\n",
    "    \n",
    "print(\"\\nAirlines data\\n\")\n",
    "print_perc_missing(airlines_raw)\n",
    "\n",
    "print(\"\\nAirports data\\n\\n\")\n",
    "print_perc_missing(airports_raw)\n",
    "\n",
    "print(\"\\nRoutes data\\n\\n\")\n",
    "print_perc_missing(routes_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "All three data sets have problems.\n",
    "\n",
    "#### Airlines data\n",
    "* Airline aliases flawed.\n",
    "* Most IATA codes are rubbish. Many NaN, and other issues.\n",
    "* ICAO codes have NaNs and other problems.\n",
    "* Further, 13% of airline callsigns are flawed.\n",
    "\n",
    "#### Airports data\n",
    "* Airport cities seems to be an issue. Not great as we'll need it later on.\n",
    "* Timezone offset too.\n",
    "* Some IATA codes are \\N --> not NaN! Need to fix this too.\n",
    "* Definitely need to fix cities if we want to use it as a key.\n",
    "* There are two unnamed columns, empty columns in the Excel sheet. We will just delete those as they only contain NaNs.\n",
    "\n",
    "#### Routes data\n",
    "\n",
    "* The `Codeshare?` column is pretty much useless.\n",
    "* The `Equipment` column has deeper issues, we'll get to that later.\n",
    "* The rest doesn't contain missing data, but there are invalid entries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we would like to check the dimensionality of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dimensions...\n",
      "(6162, 8)\n",
      "(7184, 14)\n",
      "(67663, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dta = [airlines_raw, airports_raw, routes_raw]\n",
    "\n",
    "## Print dimensions.\n",
    "print(\"\\nDimensions...\")\n",
    "list(map(lambda x: print(x.shape), all_dta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "* Routes has the largest amount of entries, ~68k rows.\n",
    "* Probably also the most scalable part of the data as new routes will be created, or existing ones updated.\n",
    "* Need to understand unit of scale in new routes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Cleaning\n",
    "\n",
    "For the cleaning process, we'll have use a mechanism that applies different cleaning strategies per each column and data set.\n",
    "\n",
    "Here are some thoughts:\n",
    "\n",
    "* Some data, like airlines IATA and ICAO code have a well-defined structure. We can use **regular expressions** to check for problems.\n",
    "* The same approach could be used to ensure data validity, e.g. for numerical columns.\n",
    "* For some missing entries, we could **back-fill** data using an API (not implemented).\n",
    "* In other cases we may have to manually fix the data, so we need to annotate whether there is a problem or not.\n",
    "* Here is an API that may help with ICAO / IATA issues: https://www.icao.int/safety/iStars/Pages/API-Data-Service.aspx\n",
    "* In practice, for anything that could be used as an index we need to ensure **uniqueness**.\n",
    "\n",
    "### ETL in production\n",
    "\n",
    "The below functions are just examples of how to clean the data in principle. It's neither complete, nor the way this should be done in production.\n",
    "\n",
    "For production I have created some sketches in the `src/pipeline/` directory using the **Luigi** framework for realizing a batch-based ETL process.\n",
    "\n",
    "### Annotation vs. automatic filling of missing entries\n",
    "\n",
    "As said above, for some data we might be able to automatically fill missing entries using APIs or other data sources (e.g. web scraping).\n",
    "\n",
    "Further, I have chose to introduce marker columns indicating whether there are missing / invalid data present. This can later be used to filter out invalid results in analytical queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper functions.\n",
    "import re\n",
    "\n",
    "def get_invalid_data(data, column_name, pattern):\n",
    "    \"\"\"\n",
    "    This helper function returns a marker Series where True indicates\n",
    "    invalid data, if the given regex pattern is NOT matched for a row in <column_name>.\n",
    "    \"\"\"\n",
    "    compiled_pattern = re.compile(pattern)\n",
    "    invalid_matcher = lambda x: bool(compiled_pattern.match(str(x)))\n",
    "    return(data[column_name].apply(invalid_matcher))\n",
    "\n",
    "def create_invalid_col_annotation(data, column_name, invalid_data):\n",
    "    \"\"\"\n",
    "    This helper function creates a new column based on a marker series.\n",
    "    \"\"\"\n",
    "    \n",
    "    data[\"invalid_\" + column_name] = invalid_data\n",
    "    return(data)\n",
    "\n",
    "\n",
    "def generic_str_strategy(data, col):\n",
    "    \"\"\"\n",
    "    Generic cleaning strategy for null / nan data (String columns).\n",
    "    \"\"\"\n",
    "    data.loc[data[\"invalid_\" + col], col] = \"MISSING\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean airlines data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_airlines_data(data):\n",
    "    \"\"\"\n",
    "    Invoke cleaning strategy for each column that needs cleaning.\n",
    "    \"\"\"\n",
    "    clean_airlines_iata(data)\n",
    "    clean_airlines_icao(data)\n",
    "    clean_airlines_callsign(data)\n",
    "    clean_airlines_country(data)\n",
    "    clean_airlines_alias(data)\n",
    "    \n",
    "    # Drop first row.\n",
    "    data = data.drop(0).reset_index(drop=True)\n",
    "    \n",
    "    return(data)\n",
    "     \n",
    "def clean_airlines_iata(data):\n",
    "    \"\"\"\n",
    "    TBD\n",
    "    \"\"\"\n",
    "    col = \"IATA Code\"\n",
    "    create_invalid_col_annotation(data, col, get_invalid_data(data, col, \"^(?![A-Z0-9]{2}).*\"))\n",
    "    generic_str_strategy(data, col)\n",
    "\n",
    "def clean_airlines_icao(data):\n",
    "    \"\"\"\n",
    "    TBD\n",
    "    \"\"\"\n",
    "    col = \"ICAO Code\"\n",
    "    create_invalid_col_annotation(data, col, get_invalid_data(data, col, \"^(?![A-Z]{3}).*\"))\n",
    "    generic_str_strategy(data, col)\n",
    "\n",
    "\n",
    "def clean_airlines_callsign(data):\n",
    "    \"\"\"\n",
    "    FIXME: This only filters out nan values, not *all* incorrect values!\n",
    "    \"\"\"\n",
    "    col = \"Airline Callsign\"\n",
    "    invalid_callsigns = data[col].isnull()\n",
    "    create_invalid_col_annotation(data, col, invalid_callsigns)\n",
    "    generic_str_strategy(data, col)\n",
    "\n",
    "def clean_airlines_country(data):\n",
    "    \"\"\"\n",
    "    FIXME: This only filters out nan values, not *all* incorrect values!\n",
    "    \"\"\"\n",
    "    col = \"Airline Country\"\n",
    "    invalid_countries = data[col].isnull()\n",
    "    create_invalid_col_annotation(data, col, invalid_countries)\n",
    "    generic_str_strategy(data, col)\n",
    "\n",
    "def clean_airlines_alias(data):\n",
    "    \"\"\"\n",
    "    FIXME: This only filters out nan values, not *all* incorrect values!\n",
    "    \"\"\"\n",
    "    col = \"Airline Alias\"\n",
    "    invalid_alias = data[col].isnull()\n",
    "    create_invalid_col_annotation(data, col, invalid_alias)\n",
    "    generic_str_strategy(data, col)\n",
    "\n",
    "# NOTE: Here we should also check if the things we use as index, e.g. Airline ID are unique!\n",
    "airlines = clean_airlines_data(airlines_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify airlines data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Airline ID                  0\n",
       "Airline Name                0\n",
       "Airline Alias               0\n",
       "IATA Code                   0\n",
       "ICAO Code                   0\n",
       "Airline Callsign            0\n",
       "Airline Country             0\n",
       "Airline Operational?        0\n",
       "invalid_IATA Code           0\n",
       "invalid_ICAO Code           0\n",
       "invalid_Airline Callsign    0\n",
       "invalid_Airline Country     0\n",
       "invalid_Airline Alias       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: Here we should also check if the things we use as index, e.g. Airline ID are unique!\n",
    "airlines = clean_airlines_data(airlines_raw)\n",
    "\n",
    "# Now all should be good (in terms of nan / null)\n",
    "airlines.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning of airports data\n",
    "\n",
    "We employ the same strategies as above.\n",
    "\n",
    "**Note**: The most contentious column in this data set is the \"City\" column, as we'll need it to run analytical queries. Also see comments in function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_airports_data(data):\n",
    "    \"\"\"\n",
    "    Clean airports data columns.\n",
    "    \"\"\"\n",
    "    clean_airports_iata(data)\n",
    "    clean_airports_icao(data)\n",
    "    clean_latitude(data)\n",
    "    clean_city(data)\n",
    "    clean_tz_offset(data)\n",
    "    clean_tz(data)\n",
    "    clean_dst(data)\n",
    "    \n",
    "    # Drop nan columns\n",
    "    data.drop(data.columns[[12, 13]], axis = 1, inplace = True)\n",
    "    return(data)\n",
    "\n",
    "\n",
    "def clean_airports_iata(data):\n",
    "    \"\"\"\n",
    "    TBD\n",
    "    \"\"\"\n",
    "    col = \"IATA Code\"\n",
    "    \n",
    "    create_invalid_col_annotation(data, col, get_invalid_data(data, col, \"^(?![A-Z]{3}).*\"))\n",
    "    generic_str_strategy(data, col)    \n",
    "\n",
    "def clean_airports_icao(data):\n",
    "    \"\"\"\n",
    "    TBD\n",
    "    \"\"\"\n",
    "    col = \"ICAO Code\"\n",
    "    create_invalid_col_annotation(data, col, get_invalid_data(data, col, \"^(?![A-Z]{4}).*\"))\n",
    "    generic_str_strategy(data, col)    \n",
    "\n",
    "def clean_city(data):\n",
    "    \"\"\"\n",
    "    This is actually crucial because we want to use City as a key!\n",
    "    \n",
    "    Also, this particular column seems to be a mess.\n",
    "    \n",
    "    Real-world considerations:\n",
    "        - UTF conversion issues?\n",
    "        - Invalid values....\n",
    "    \"\"\"\n",
    "    col = \"Airport City\"\n",
    "    invalid_city = data[col].isnull()\n",
    "    create_invalid_col_annotation(data, col, invalid_city)\n",
    "    generic_str_strategy(data, col)\n",
    "\n",
    "\n",
    "def clean_tz_offset(data):\n",
    "    \"\"\"\n",
    "    FIXME: This only filters out nan values, not *all* incorrect values!\n",
    "    \"\"\"\n",
    "    col = \"Timezone Offset\"\n",
    "    invalid_tz_off = data[col].isnull()\n",
    "    create_invalid_col_annotation(data, col, invalid_tz_off)\n",
    "    generic_str_strategy(data, col)\n",
    "\n",
    "def clean_tz(data):\n",
    "    \"\"\"\n",
    "    FIXME: This only filters out nan values, not *all* incorrect values!\n",
    "    \"\"\"\n",
    "    col = \"Timezone\"\n",
    "    invalid_tz = data[col].isnull()\n",
    "    create_invalid_col_annotation(data, col, invalid_tz)\n",
    "    generic_str_strategy(data, col)\n",
    "\n",
    "def clean_dst(data):\n",
    "    \"\"\"\n",
    "    FIXME: This only filters out nan values, not *all* incorrect values!\n",
    "    \"\"\"\n",
    "    col = \"DST\"\n",
    "    invalid_dst = data[col].isnull()\n",
    "    create_invalid_col_annotation(data, col, invalid_dst)\n",
    "    generic_str_strategy(data, col)\n",
    "\n",
    "def clean_latitude(data):\n",
    "    \"\"\"\n",
    "    FIXME: This only filters out nan values, not *all* incorrect values!\n",
    "    \"\"\"\n",
    "    col = \"Latitude\"\n",
    "    inv = data[col].isnull()\n",
    "    create_invalid_col_annotation(data, col, inv)\n",
    "    generic_str_strategy(data, col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify airports data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Airport ID                 0\n",
       "Airport Name               0\n",
       "Airport City               0\n",
       "Airport Country            0\n",
       "IATA Code                  0\n",
       "ICAO Code                  0\n",
       "Latitude                   0\n",
       "Longitude                  0\n",
       "Altitude                   0\n",
       "Timezone Offset            0\n",
       "DST                        0\n",
       "Timezone                   0\n",
       "invalid_IATA Code          0\n",
       "invalid_ICAO Code          0\n",
       "invalid_Latitude           0\n",
       "invalid_Airport City       0\n",
       "invalid_Timezone Offset    0\n",
       "invalid_Timezone           0\n",
       "invalid_DST                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airports = clean_airports_data(airports_raw)\n",
    "airports.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning routes data\n",
    "\n",
    "One of the issues here is the **equipment column** which may hold multiple entries. \n",
    "\n",
    "As we want to be able to query for equipment type, the strategy is to create one boolean-type column for each equipment type indicating whether this aircraft is being used on the present route, or not (True / False).\n",
    "\n",
    "There are other issues similar to the other data sets (like `\\N`) etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_routes_data(data):\n",
    "    \"\"\"\n",
    "    Invokes all the column cleaning strategies for the routes data.\n",
    "    \"\"\"\n",
    "    data = clean_equipment(data)\n",
    "    clean_airline(data)\n",
    "    clean_destination_airport(data)\n",
    "    clean_codeshare(data)\n",
    "    clean_no_stops(data)\n",
    "    data = clean_source_airport_id(data)\n",
    "    data = clean_destination_airport_id(data)\n",
    "    return(data)\n",
    "    \n",
    "\n",
    "def clean_airline(data):\n",
    "    \"\"\"\n",
    "    FIXME: This only filters out nan values, not *all* incorrect values!\n",
    "    \"\"\"\n",
    "    col = \"Airline\"\n",
    "    invalid = data[col].isnull()\n",
    "    create_invalid_col_annotation(data, col, invalid)\n",
    "    generic_str_strategy(data, col)    \n",
    "    \n",
    "def clean_destination_airport(data):\n",
    "    \"\"\"\n",
    "    FIXME: This only filters out nan values, not *all* incorrect values!\n",
    "    \"\"\"\n",
    "    col = \"Destination Airport\"\n",
    "    invalid = data[col].isnull()\n",
    "    create_invalid_col_annotation(data, col, invalid)\n",
    "    generic_str_strategy(data, col) \n",
    "    \n",
    "def clean_codeshare(data):\n",
    "    \"\"\"\n",
    "    FIXME: This only filters out nan values, not *all* incorrect values!\n",
    "    \"\"\"\n",
    "    col = \"Codeshare?\"\n",
    "    invalid = data[col].isnull()\n",
    "    create_invalid_col_annotation(data, col, invalid)\n",
    "    generic_str_strategy(data, col)  \n",
    "\n",
    "def clean_no_stops(data):\n",
    "    \"\"\"\n",
    "    FIXME: This only filters out nan values, not *all* incorrect values!\n",
    "    \"\"\"\n",
    "    col = \"No Stops\"\n",
    "    invalid = data[col].isnull()\n",
    "    create_invalid_col_annotation(data, col, invalid)\n",
    "    generic_str_strategy(data, col)  \n",
    "\n",
    "def clean_equipment(data):\n",
    "    \"\"\"\n",
    "    This function cleans the equipment column.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert this column to a string type.\n",
    "    data = data.astype({\"Equipment\": np.str})\n",
    "    \n",
    "    # Flatten a nested list.\n",
    "    # FIXME: Should consider using itertools for Python >= 2.6\n",
    "    # https://docs.python.org/3/library/itertools.html#itertools.product\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "    equip_raw = data[\"Equipment\"]\n",
    "\n",
    "    # Generate a list of unique equpment types.\n",
    "    # FIXME: Might need to consider more than one space; remove empty strings / nan.\n",
    "    equip_raw = [e.split(\" \") for e in equip_raw]    \n",
    "    equip = np.unique(flatten(equip_raw))\n",
    "    equip[(equip == \"\") |  (equip == \"nan\")] = \"MISSING\"\n",
    "    \n",
    "    # Generate column names.\n",
    "    prepend_eq = np.vectorize(lambda x: \"eq_\" + x)\n",
    "    equip_cols = prepend_eq(equip)\n",
    "    \n",
    "    # Now add a boolean column for each eq. type.\n",
    "    tmp = data.reindex(columns = equip_cols)\n",
    "    tmp.fillna(value = False, inplace = True)\n",
    "    tmp = tmp.astype(np.bool)\n",
    "    \n",
    "    # Join\n",
    "    data.join(tmp)\n",
    "    \n",
    "    # Set to true if equip is present.\n",
    "    # FIXME: Perhaps not the most elegant solution.\n",
    "    for col in equip:\n",
    "        data[\"eq_\" + col] = data[\"Equipment\"].str.contains(col)\n",
    "    \n",
    "    return(data)\n",
    "    \n",
    "def clean_airport_id(data, col_name):\n",
    "    \"\"\"\n",
    "    Generic function to clean airport IDs.\n",
    "    \"\"\"\n",
    "    non_num = re.compile(\"^(?![0-9]+).*\")\n",
    "    invalid_ids = lambda x: bool(non_num.match(str(x)))\n",
    "    inv_ids = data[col_name].apply(invalid_ids)\n",
    "    data.loc[inv_ids, col_name] =  \"-1\"\n",
    "    # Convert to int!    \n",
    "\n",
    "def clean_source_airport_id(data):\n",
    "    \"\"\"\n",
    "    Clean the Source ID column, so we can use it as a key.\n",
    "    \"\"\"\n",
    "    clean_airport_id(data, \"Source Airport ID\")\n",
    "    return(data.astype({\"Source Airport ID\": np.int}))\n",
    "    \n",
    "def clean_destination_airport_id(data):\n",
    "    \"\"\"\n",
    "    Clean the Destination ID column, so we can use it as a key.\n",
    "    \"\"\"\n",
    "    clean_airport_id(data, \"Destination Airport ID\")\n",
    "    return(data.astype({\"Destination Airport ID\": np.int}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify routes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Airline                        0\n",
       "Airline ID                     0\n",
       "Source Airport                 0\n",
       "Source Airport ID              0\n",
       "Destination Airport            0\n",
       "Destination Airport ID         0\n",
       "Codeshare?                     0\n",
       "No Stops                       0\n",
       "Equipment                      0\n",
       "eq_MISSI                       0\n",
       "eq_100                         0\n",
       "eq_141                         0\n",
       "eq_142                         0\n",
       "eq_143                         0\n",
       "eq_146                         0\n",
       "eq_310                         0\n",
       "eq_313                         0\n",
       "eq_318                         0\n",
       "eq_319                         0\n",
       "eq_320                         0\n",
       "eq_321                         0\n",
       "eq_326                         0\n",
       "eq_329                         0\n",
       "eq_32A                         0\n",
       "eq_32B                         0\n",
       "eq_32C                         0\n",
       "eq_32S                         0\n",
       "eq_330                         0\n",
       "eq_332                         0\n",
       "eq_333                         0\n",
       "                              ..\n",
       "eq_M80                         0\n",
       "eq_M82                         0\n",
       "eq_M83                         0\n",
       "eq_M87                         0\n",
       "eq_M88                         0\n",
       "eq_M90                         0\n",
       "eq_MA6                         0\n",
       "eq_NDE                         0\n",
       "eq_PA1                         0\n",
       "eq_PA2                         0\n",
       "eq_PAG                         0\n",
       "eq_PL2                         0\n",
       "eq_S20                         0\n",
       "eq_S76                         0\n",
       "eq_SF3                         0\n",
       "eq_SFB                         0\n",
       "eq_SH6                         0\n",
       "eq_SU9                         0\n",
       "eq_SWM                         0\n",
       "eq_T20                         0\n",
       "eq_TU3                         0\n",
       "eq_TU5                         0\n",
       "eq_YK2                         0\n",
       "eq_YK4                         0\n",
       "eq_YN2                         0\n",
       "eq_YN7                         0\n",
       "invalid_Airline                0\n",
       "invalid_Destination Airport    0\n",
       "invalid_Codeshare?             0\n",
       "invalid_No Stops               0\n",
       "Length: 203, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "routes = clean_routes_data(routes_raw)\n",
    "\n",
    "routes.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data saving / ingestion\n",
    "\n",
    "We can now export the cleaned data either as CSV files or quickly load them into a Postgres DB.\n",
    "\n",
    "\n",
    "### Data Warehouse in production\n",
    "\n",
    "Again, the chosen set-up here is clearly not how to deploy this in production. Check the project's `README.md` and the notes in `src/db` for some technology suggestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching package metadata ...........\n",
      "Solving package specifications: .\n",
      "\n",
      "Package plan for installation in environment /opt/conda:\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "    krb5:     1.14.2-hcdc1b81_6     \n",
      "    libpq:    9.6.6-h4e02ad2_0      \n",
      "    psycopg2: 2.7.3.2-py36h2b1659c_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "    anaconda: 5.0.1-py36hd30a520_1   --> custom-py36hbbc8b67_0\n",
      "    conda:    4.3.30-py36h5d9f9f4_0  --> 4.4.8-py36_0         \n",
      "    openssl:  1.0.2l-h077ae2c_5      --> 1.0.2n-hb7f436b_0    \n",
      "    pycosat:  0.6.2-py36h1a0ea17_1   --> 0.6.3-py36h0a5515d_0 \n",
      "\n",
      "openssl-1.0.2n 100% |################################| Time: 0:00:08 405.40 kB/s\n",
      "krb5-1.14.2-hc 100% |################################| Time: 0:00:02 531.27 kB/s\n",
      "libpq-9.6.6-h4 100% |################################| Time: 0:00:00   1.77 MB/s\n",
      "anaconda-custo 100% |################################| Time: 0:00:00  13.75 MB/s\n",
      "psycopg2-2.7.3 100% |################################| Time: 0:00:00   1.77 MB/s\n",
      "pycosat-0.6.3- 100% |################################| Time: 0:00:00   1.85 MB/s\n",
      "conda-4.4.8-py 100% |################################| Time: 0:00:00   1.09 MB/s\n"
     ]
    }
   ],
   "source": [
    "!conda install psycopg2 -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to PSQL URL: postgresql://postgres:admin@db:5432/airlines_data\n"
     ]
    }
   ],
   "source": [
    "# Save processed data to CSV.\n",
    "\n",
    "# Un-comment to enable.\n",
    "#\n",
    "# airlines.to_csv(PROJ_PATH + DATA_OUT_PREFIX + \"airlines.csv\", index = False)\n",
    "# airports.to_csv(PROJ_PATH + DATA_OUT_PREFIX + \"airports.csv\", index = False)\n",
    "# routes.to_csv(PROJ_PATH + DATA_OUT_PREFIX + \"routes.csv\", index = False)\n",
    "\n",
    "\n",
    "#\n",
    "# Ingest directly to database.\n",
    "#\n",
    "# FIXME: Should consider converting column names to exclude whitespaces.\n",
    "# FIXME: Supply data schema.\n",
    "\n",
    "# Note: May need to install additional packages:\n",
    "# !conda install psycopg2\n",
    "# Note: Ensure DB has been created beforehand.\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "PSQL_HOST = \"db\"\n",
    "PSQL_USER = \"postgres\"\n",
    "PSQL_PW = \"admin\"\n",
    "PSQL_PORT = \"5432\"\n",
    "DB_NAME = \"airlines_data\"\n",
    "\n",
    "DB_URL = \"postgresql://\" + PSQL_USER + \":\" + PSQL_PW + \"@\" + PSQL_HOST + \":\" + PSQL_PORT + \"/\" + DB_NAME\n",
    "\n",
    "print(\"Connecting to PSQL URL: \" + DB_URL)\n",
    "\n",
    "# Create connection.\n",
    "engine = create_engine(DB_URL)\n",
    "\n",
    "# Dump data using default schema and name.\n",
    "airlines.to_sql(name = \"airlines\", con = engine, if_exists = \"replace\")\n",
    "airports.to_sql(name = \"airports\", con = engine, if_exists = \"replace\")\n",
    "\n",
    "# This one will take a bit of time.\n",
    "# FIXME: Supply schema!\n",
    "routes.to_sql(name = \"routes\", con = engine, if_exists = \"replace\")\n",
    "\n",
    "# Close connection.\n",
    "con = engine.raw_connection()\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "\n",
    "In the below I get to some of the questions regarding the data.\n",
    "\n",
    "The results are obtained using Pandas Dataframe filtering. The same could be achieved using SQL queries, see the `src/db` folder for examples like:\n",
    "\n",
    "\n",
    "```sql\n",
    "-- How many routes do we have out of London, UK?\n",
    "select count(*) from airports a \n",
    "\tjoin routes r on a.\"Airport ID\" = r.\"Source Airport ID\" \n",
    "\twhere a.\"Airport ID\" in\n",
    "\t(select \"Airport ID\" from airports where \"Airport City\" like '%London' and \"Airport Country\" like '%United Kingdom')\n",
    "```\n",
    "\n",
    "### 1. Which aircraft type is the most common on routes out of London?\n",
    "\n",
    "I'm assuming London, UK (there is a London in the UK, Canada and US)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eq_320    352\n",
      "eq_319    303\n",
      "eq_738    162\n",
      "eq_777    134\n",
      "eq_321    128\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## Firstly, has any of the missing cities to do with London?\n",
    "airports[\"Airport Name\"].str.contains(\"London\")\n",
    "london_in_name = airports[\"Airport Name\"].str.contains(\"London\")\n",
    "#print(airports[london_in_name])\n",
    "## Seemingly so, but wouldn't work with any city.\n",
    "## Bottom line is City really needs to be cleaned properly.\n",
    "\n",
    "\n",
    "## Get all the routes out of London.\n",
    "\n",
    "## Issue here: Both City and Country might be flawed!\n",
    "london_airport_ids = airports[(airports[\"Airport City\"] == \"London\") & (airports[\"Airport Country\"] == \"United Kingdom\")][\"Airport ID\"]\n",
    "\n",
    "## Get all the routes out of London.\n",
    "routes_out_of_london = routes[routes[\"Source Airport ID\"].isin(list(london_airport_ids))]\n",
    "\n",
    "## Get the most frequently used equipment type.\n",
    "most_freq = routes_out_of_london.filter(regex = \"^eq_.*\").sum().sort_values(ascending = False)\n",
    "\n",
    "## RES: A320\n",
    "print(most_freq.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "\n",
    "Looks like the Airbus A320 is the most frequent equipment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Which airline has the most routes into JFK?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2007    Delta Air Lines\n",
      "Name: Airline Name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "## Need ID of destination JFK. We will use IATA.\n",
    "## NOTE: This means IATA must be ok.\n",
    "jfk_id = int(airports[(airports[\"IATA Code\"] == \"JFK\")][\"Airport ID\"].values)\n",
    "\n",
    "## Get all the airlines IDs with jfk_id as destination.\n",
    "airline_ids_into_jfk = routes[routes[\"Destination Airport ID\"] == jfk_id][\"Airline ID\"]\n",
    "\n",
    "## Get ID with most flights.\n",
    "most_routes_id = airline_ids_into_jfk.value_counts().index[0]\n",
    "\n",
    "## Look up name.\n",
    "## RES: Delta Airlines.\n",
    "print(airlines[airlines[\"Airline ID\"] == most_routes_id][\"Airline Name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "\n",
    "Delta Airlines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
